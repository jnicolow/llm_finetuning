{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,\"What did the bartender say to the jumper cables? You better not try to start anything.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join('data', 'joke', 'data')) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if 'ID' in line: continue\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_id, joke = tuple(line.split(',\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jnicolow\\AppData\\Local\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'nlp.datasets' from 'c:\\\\Users\\\\jnicolow\\\\Documents\\\\courses\\\\fall2024\\\\ICS-661\\\\llm_finetuning\\\\nlp\\\\datasets.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from nlp import datasets\n",
    "import importlib\n",
    "importlib.reload(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1459 training examples\n",
      "there are 163 test examples\n",
      "There are three types of people in this world. Those who can count and those who can't.\"\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "There are three types of people in this world. Those who can count and those who can't.\"\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(datasets)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token}) # there is no default\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "dataset = datasets.JokesDataset(filepath=os.path.join('data', 'joke', 'data'), tokenizer=tokenizer)\n",
    "train_ratio = 0.9\n",
    "eval_ratio = 1 - train_ratio\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "eval_size = total_size - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f'there are {len(train_dataset)} training examples')\n",
    "print(f'there are {len(eval_dataset)} test examples')\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(tokenizer.decode(batch['input_ids'][0]))\n",
    "    print(tokenizer.decode(batch['target_ids'][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # resize the model's token embeddings to accommodate the new padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.8616\n",
      "Epoch 2/15, Loss: 0.6898\n",
      "Epoch 3/15, Loss: 0.6149\n",
      "Epoch 4/15, Loss: 0.5517\n",
      "Epoch 5/15, Loss: 0.4947\n",
      "Epoch 6/15, Loss: 0.4368\n",
      "Epoch 7/15, Loss: 0.3845\n",
      "Epoch 8/15, Loss: 0.3376\n",
      "Epoch 9/15, Loss: 0.2938\n",
      "Epoch 10/15, Loss: 0.2537\n",
      "Epoch 11/15, Loss: 0.2229\n",
      "Epoch 12/15, Loss: 0.1957\n",
      "Epoch 13/15, Loss: 0.1736\n",
      "Epoch 14/15, Loss: 0.1540\n",
      "Epoch 15/15, Loss: 0.1413\n",
      "Example 1: Where do you go to weigh a pie? Somewhere over the rainbow weigh a pie. (sounds like way up high)\"\n",
      "\n",
      "Example 2: What is a rocket's favorite meal? Launch! Another one from my 9 year old.\"\n",
      "\n",
      "Evaluation Loss: 1.1074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " 'models/fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " 'models/fine_tuned_gpt2\\\\vocab.json',\n",
       " 'models/fine_tuned_gpt2\\\\merges.txt',\n",
       " 'models/fine_tuned_gpt2\\\\added_tokens.json',\n",
       " 'models/fine_tuned_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu') # second gpu has more mem\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)  \n",
    "\n",
    "def train_model(model, train_loader, optimizer, device, num_epochs=3):\n",
    "    model.train()  \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = input_ids.clone()  \n",
    "            labels[labels == '[PAD]'] = -100  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss  \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, eval_loader, device):\n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(eval_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = input_ids.clone()  \n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            if batch_index == 0:  \n",
    "                for i in range(min(2, input_ids.size(0))):  \n",
    "                    decoded_input = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                    print(f'Example {i + 1}: {decoded_input}')\n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    print(f'Evaluation Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "train_model(model, train_loader, optimizer, device, num_epochs=15)\n",
    "\n",
    "evaluate_model(model, eval_loader, device)\n",
    "\n",
    "# save model and tokenizer\n",
    "model.save_pretrained('models/fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('models/fine_tuned_gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Words: ['cat', 'dog', 'banana']\n",
      "Generated Joke: cat dog banana walk into a bar The bartender says, \"\"What's with the banana walk?\"\" And the banana walks away.\"\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/fine_tuned_gpt2\") # load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('models/fine_tuned_gpt2') # load model\n",
    "model.eval()  \n",
    "\n",
    "def generate_random_words(word_list, num_words=3):\n",
    "    return random.sample(word_list, num_words)\n",
    "\n",
    "def generate_joke(starting_words, max_length=50):\n",
    "    prompt = ' '.join(starting_words) \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    generated_ids = output[0]  # get the first output (for a single sample)\n",
    "    generated_ids = generated_ids.tolist()  # convert tensor to list\n",
    "    \n",
    "    \n",
    "    joke = tokenizer.decode(generated_ids, skip_special_tokens=False)\n",
    "    return joke\n",
    "\n",
    "\n",
    "word_list = ['cat', 'dog', 'fish', 'funny', 'computer', 'banana', 'rocket', 'pizza', 'monkey', 'joke'] # random words are generated from this\n",
    "\n",
    "\n",
    "random_words = generate_random_words(word_list)\n",
    "\n",
    "# random_words = ['What do you']\n",
    "joke = generate_joke(random_words)\n",
    "print(\"Starting Words:\", random_words)\n",
    "print(\"Generated Joke:\", joke)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
